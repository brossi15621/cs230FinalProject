{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "import math as math\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in our processed dataset of spectrograms and popularity classes\n",
    "\n",
    "processed_dataset = pd.read_csv('processed_dataset.csv')  \n",
    "processed_dataset = processed_dataset.values\n",
    "print(processed_dataset.shape)\n",
    "image_list = []\n",
    "#build matrix of spectrograms\n",
    "for item in processed_dataset:\n",
    "    #read in image, convert to matrix of pixels, chop off alpha values, convert to gray-scale\n",
    "    image_file = item[3]\n",
    "    image = imageio.imread(image_file)\n",
    "    image = np.dot(image[...,:4], [0.299, 0.587, 0.114, 0])\n",
    "    image_list.append(image)\n",
    "print(len(image_list))\n",
    "image_data = np.stack(image_list, axis=0)\n",
    "print(image_data.shape)\n",
    "\n",
    "labels_data = processed_dataset[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute distribution of train data\n",
    "\n",
    "num_low_pop = 0\n",
    "num_med_pop = 0\n",
    "num_high_pop = 0\n",
    "for item in labels_data:\n",
    "    if int(item) == 0:\n",
    "        num_low_pop += 1\n",
    "    elif int(item) == 1:\n",
    "        num_med_pop += 1\n",
    "    else:\n",
    "        num_high_pop += 1\n",
    "    \n",
    "percent_low = float(num_low_pop)/(num_low_pop + num_med_pop + num_high_pop)\n",
    "percent_med = float(num_med_pop)/(num_low_pop + num_med_pop + num_high_pop)\n",
    "percent_high = float(num_high_pop)/(num_low_pop + num_med_pop + num_high_pop)\n",
    "\n",
    "print('percent of data that is low population:', percent_low)\n",
    "print('percent of data that is med population:', percent_med)\n",
    "print('percent of data that is high population:', percent_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, dev, and test sets. Split into 80%, 10%, 10% respectively\n",
    "\n",
    "train_data_end_index = int(image_data.shape[0] * 0.8)\n",
    "dev_data_end_index = int(image_data.shape[0] * 0.9)\n",
    "\n",
    "train_data = image_data[0:train_data_end_index]\n",
    "dev_data = image_data[train_data_end_index:dev_data_end_index]\n",
    "test_data = image_data[dev_data_end_index:]\n",
    "\n",
    "train_labels = labels_data[0:train_data_end_index]\n",
    "dev_labels = labels_data[train_data_end_index:dev_data_end_index]\n",
    "test_labels = labels_data[dev_data_end_index:]\n",
    "\n",
    "train_labels = np.eye(3)[train_labels.astype(int)]\n",
    "dev_labels = np.eye(3)[dev_labels.astype(int)]\n",
    "test_labels = np.eye(3)[test_labels.astype(int)]\n",
    "\n",
    "print(train_data.shape)\n",
    "print(dev_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "print(train_labels.shape)\n",
    "print(dev_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute distribution of train data\n",
    "\n",
    "num_low_pop = 0\n",
    "num_med_pop = 0\n",
    "num_high_pop = 0\n",
    "for item in train_labels:\n",
    "    if item[0] == 1:\n",
    "        num_low_pop += 1\n",
    "    elif item[1] == 1:\n",
    "        num_med_pop += 1\n",
    "    else:\n",
    "        num_high_pop += 1\n",
    "    \n",
    "percent_train_low = float(num_low_pop)/(num_low_pop + num_med_pop + num_high_pop)\n",
    "percent_train_med = float(num_med_pop)/(num_low_pop + num_med_pop + num_high_pop)\n",
    "percent_train_high = float(num_high_pop)/(num_low_pop + num_med_pop + num_high_pop)\n",
    "\n",
    "print('percent of training data that is low population:', percent_train_low)\n",
    "print('percent of training data that is med population:', percent_train_med)\n",
    "print('percent of training data that is high population:', percent_train_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute distribution of dev data\n",
    "\n",
    "num_low_pop = 0\n",
    "num_med_pop = 0\n",
    "num_high_pop = 0\n",
    "for item in dev_labels:\n",
    "    if item[0] == 1:\n",
    "        num_low_pop += 1\n",
    "    elif item[1] == 1:\n",
    "        num_med_pop += 1\n",
    "    else:\n",
    "        num_high_pop += 1\n",
    "    \n",
    "percent_dev_low = float(num_low_pop)/(num_low_pop + num_med_pop + num_high_pop)\n",
    "percent_dev_med = float(num_med_pop)/(num_low_pop + num_med_pop + num_high_pop)\n",
    "percent_dev_high = float(num_high_pop)/(num_low_pop + num_med_pop + num_high_pop)\n",
    "\n",
    "print('percent of dev data that is low population:', percent_dev_low)\n",
    "print('percent of dev data that is med population:', percent_dev_med)\n",
    "print('percent of dev data that is high population:', percent_dev_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute distribution of test data\n",
    "\n",
    "num_low_pop = 0\n",
    "num_med_pop = 0\n",
    "num_high_pop = 0\n",
    "for item in test_labels:\n",
    "    if item[0] == 1:\n",
    "        num_low_pop += 1\n",
    "    elif item[1] == 1:\n",
    "        num_med_pop += 1\n",
    "    else:\n",
    "        num_high_pop += 1\n",
    "    \n",
    "percent_test_low = float(num_low_pop)/(num_low_pop + num_med_pop + num_high_pop)\n",
    "percent_test_med = float(num_med_pop)/(num_low_pop + num_med_pop + num_high_pop)\n",
    "percent_test_high = float(num_high_pop)/(num_low_pop + num_med_pop + num_high_pop)\n",
    "\n",
    "print('percent of test data that is low population:', percent_test_low)\n",
    "print('percent of test data that is med population:', percent_test_med)\n",
    "print('percent of test data that is high population:', percent_test_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 example gray-scaled image\n",
    "plt.imshow(train_data[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.reshape(train_data, (train_data.shape[0], train_data.shape[1], train_data.shape[2], 1))\n",
    "dev_data = np.reshape(dev_data, (dev_data.shape[0], dev_data.shape[1], dev_data.shape[2], 1))\n",
    "test_data = np.reshape(test_data, (test_data.shape[0], test_data.shape[1],test_data.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf tensors for model\n",
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    # x will contain train examples\n",
    "    features = tf.placeholder(tf.float32, [None, n_H0, n_W0, n_C0], name='x')\n",
    "    # labels will contain the true popularity scores\n",
    "    labels = tf.placeholder(tf.float32, shape=[None, n_y], name='y_true')\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = create_placeholders(train_data.shape[1], train_data.shape[2], 1, 3)\n",
    "print (\"X =\" + str(X))\n",
    "print (\"Y =\" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    tf.set_random_seed(1)\n",
    "\n",
    "    W1 = tf.get_variable(\"W1\", [3, 5, 1, 64], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W2 = tf.get_variable(\"W2\", [3, 5, 64, 128], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W3 = tf.get_variable(\"W3\", [2, 2, 128, 256], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W4 = tf.get_variable(\"W4\", [2, 2, 256, 512], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "\n",
    "    parameters = {\"W1\":W1,\n",
    "                  \"W2\":W2,\n",
    "                  \"W3\":W3,\n",
    "                  \"W4\":W4}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogation(X, parameters, training):\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    W3 = parameters['W3']\n",
    "    W4 = parameters['W4']\n",
    "    \n",
    "    # first convolutional layer:     strides = 1 ... [1,s,s,1]\n",
    "    Z1 = tf.nn.conv2d(X,W1, strides = [1,1,1,1], padding='VALID')\n",
    "    # relu activation\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    # maxpool:     strides = 2... [1,s,s,1], window = 2x2...[1,f,f,1]\n",
    "    P1 = tf.nn.max_pool(A1, strides = [1,2,2,1], ksize=[1,2,2,1], padding='VALID')\n",
    "\n",
    "    # second convolutional layers:     strides = 1 ... [1,s,s,1]\n",
    "    Z2 = tf.nn.conv2d(P1,W2, strides = [1,1,1,1], padding='VALID')\n",
    "    # relu activation\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    # maxpool:     strides = 2... [1,s,s,1], window = 2x2...[1,f,f,1]\n",
    "    P2 = tf.nn.max_pool(A2, strides=[1,2,2,1], ksize=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    # third convolutional layer:     strides = 1 ... [1,s,s,1]\n",
    "    Z3 = tf.nn.conv2d(P2,W3, strides = [1,1,1,1], padding='VALID')\n",
    "    # relu activation\n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    # maxpool:     strides = 2... [1,s,s,1], window = 2x2...[1,f,f,1]\n",
    "    P3 = tf.nn.max_pool(A3, strides = [1,2,2,1], ksize=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    # fourth convolutional layer:     strides = 1 ... [1,s,s,1]\n",
    "    Z4 = tf.nn.conv2d(P3,W4, strides = [1,1,1,1], padding='VALID')\n",
    "    # relu activation\n",
    "    A4 = tf.nn.relu(Z4)\n",
    "    # maxpool:     strides = 2... [1,s,s,1], window = 2x2...[1,f,f,1]\n",
    "    P4 = tf.nn.max_pool(A4, strides = [1,2,2,1], ksize=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Flatten\n",
    "    P5 = tf.contrib.layers.flatten(P4)\n",
    "    \n",
    "    # fully connected layer with dropout\n",
    "    fc_1 = tf.layers.dense(inputs=P5, units=1024, activation=tf.nn.relu)\n",
    "    fc_1_dropout = tf.layers.dropout(\n",
    "      inputs=fc_1, rate=0.4, training=training)\n",
    "    \n",
    "    Z5 = tf.contrib.layers.fully_connected(fc_1_dropout, 3, activation_fn=tf.nn.softmax)\n",
    "    \n",
    "    return Z5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 1 instance of the network\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    np.random.seed(1)\n",
    "    X, Y = create_placeholders(train_data.shape[1], train_data.shape[2], 1, 3)\n",
    "    parameters = initialize_parameters()\n",
    "    Z5 = forward_propogation(X, parameters, True)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    a = sess.run(Z5, {X: train_data[0:2], Y: train_labels[0:2]})\n",
    "    print(\"Z5 = \" + str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z5, Y):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z5, labels = Y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "\tm = X.shape[0]\n",
    "\tmini_batches = []\n",
    "\tnp.random.seed(seed)\n",
    "\n",
    "\t# Step 1:\n",
    "\tpermutation = list(np.random.permutation(m))\n",
    "\tshuffled_X = X[permutation,:,:,:]\n",
    "\tshuffled_Y = Y[permutation,:]\n",
    "\n",
    "\t# Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "\tnum_complete_minibatches = math.floor(m/mini_batch_size)\n",
    "\tfor k in range(0, num_complete_minibatches):\n",
    "\t\tmini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
    "\t\tmini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "\t\tmini_batch = (mini_batch_X, mini_batch_Y)\n",
    "\t\tmini_batches.append(mini_batch)\n",
    "\t# Handling the end case (last mini-batch < mini_batch_size)\n",
    "\tif m % mini_batch_size != 0:\n",
    "\t\tmini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n",
    "\t\tmini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
    "\t\tmini_batch = (mini_batch_X, mini_batch_Y)\n",
    "\t\tmini_batches.append(mini_batch)\n",
    "\treturn mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.005,\n",
    "          num_epochs = 100, minibatch_size = 16, print_cost = True):\n",
    "    \n",
    "    ops.reset_default_graph()\n",
    "    tf.set_random_seed(1)\n",
    "    seed = 3\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape\n",
    "    n_y = Y_train.shape[1]\n",
    "    costs = []\n",
    "    \n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "    parameters = initialize_parameters()\n",
    "    Z5 = forward_propogation(X, parameters, True)\n",
    "    \n",
    "    cost = compute_cost(Z5,Y)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            minibatch_cost = 0\n",
    "            num_minibatches = int(m / minibatch_size)\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            \n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                _ , temp_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "            \n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "                \n",
    "        # plot the cost \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "        \n",
    "        predict_op = tf.argmax(Z5, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y,1))\n",
    "            \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        tf.train.start_queue_runners(sess)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "\n",
    "        print(\"Train Accuracy\", train_accuracy)\n",
    "        print(\"Test Accuracy\", test_accuracy)\n",
    "            \n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with dev set\n",
    "_, _, parameters = model(train_data, train_labels, dev_data, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### The following is performance for milestone (~1,000 examples) #####\n",
    "\n",
    "# current distributions of train and dev set (low, med, high): \n",
    "# train = (16.21%, 46.651%, 37.13%)\n",
    "# dev = (24%, 48%, 28%)\n",
    "\n",
    "# NOTE: We need more data before we can really determine what strategies to use (e.g. deeper network, regularization, etc.)\n",
    "\n",
    "\n",
    "# learning_rate = .00005, epochs = 200, minibatch_size = 3: train = 60.5%, test = 44%\n",
    "# learning_rate = .00005, epochs = 100, minibatch_size = 3: train = 51.47%, test = 45%\n",
    "# learning_rate = .0001, epochs = 100, minibatch_size = 3: train = 59.34%, test = 43%\n",
    "# learning_rate = .0001, epochs = 100, minibatch_size = 5: train = 53.7%, test = 35%\n",
    "# learning_rate = .0001, epochs = 100, minibatch_size = 1: train = 63.69, test = 41%\n",
    "# learning_rate = .005, epochs = 100, minibatch_size = 3: train = 46.651% , test = 47%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
